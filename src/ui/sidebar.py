"""
æ¨¡å—åç§°: Sidebar Component (ä¾§è¾¹æ ç»„ä»¶)
åŠŸèƒ½æè¿°:

    æ¸²æŸ“åº”ç”¨çš„å·¦ä¾§æ§åˆ¶é¢æ¿ã€‚
    åŒ…å«æ¨¡å‹é€‰æ‹©ã€çŸ¥è¯†åº“ç®¡ç† (ä¸Šä¼ /é‡å»º)ã€ç¼“å­˜æ¸…ç†ç­‰ç³»ç»Ÿçº§æ“ä½œå…¥å£ã€‚

è®¾è®¡ç†å¿µ:

    1.  **åŠŸèƒ½èšåˆ**: å°†é…ç½®å’Œç®¡ç†ç±»åŠŸèƒ½é›†ä¸­åœ¨ä¾§è¾¹æ ï¼Œä¿æŒä¸»ç•Œé¢ (Main Content) ä¸“æ³¨äºè¯Šæ–­ä¸šåŠ¡ã€‚
    2.  **å³æ—¶åé¦ˆ**: æ“ä½œ (å¦‚åˆ‡æ¢æ¨¡å‹) ç«‹å³ç”Ÿæ•ˆï¼Œé€šå¸¸é€šè¿‡ä¿®æ”¹ç¯å¢ƒå˜é‡æˆ– Session State å®ç°ã€‚
    3.  **çŠ¶æ€å¯è§†**: æ˜¾ç¤ºå½“å‰è¿æ¥çš„æ¨¡å‹ã€æ•°æ®åº“çŠ¶æ€ç­‰ä¿¡æ¯ã€‚

çº¿ç¨‹å®‰å…¨æ€§:

    - ä¾èµ– Streamlit çš„æ¸²æŸ“çº¿ç¨‹ï¼Œæ“ä½œ Session State éœ€æ³¨æ„å¹¶å‘ (ä½†åœ¨ Streamlit ä¸­é€šå¸¸æ˜¯å•çº¿ç¨‹æ¨¡å‹)ã€‚

ä¾èµ–å…³ç³»:

    - `streamlit`: UI æ¡†æ¶ã€‚
    - `src.core.settings`: è¯»å–å’Œä¿®æ”¹é…ç½®ã€‚
"""

import os
import streamlit as st

# [å®šä¹‰å‡½æ•°] ############################################################################################################
# [UI-æ¸²æŸ“ä¾§è¾¹æ ] =========================================================================================================
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ ç»„ä»¶"""
    with st.sidebar:
        st.subheader("ğŸ¤– é€‰æ‹©å¤§æ¨¡å‹")
        
        # [step1] æ¨¡å‹åˆ‡æ¢åŠŸèƒ½
        model_options = {
            "Qwen-Turbo (é€šä¹‰åƒé—®)": "qwen",
            "Baichuan M2 (ç™¾å·)": "baichuan",
            "Ollama Service (æœ¬åœ°æœåŠ¡)": "ollama",
            "HuggingFace Native (åŸç”ŸåŠ è½½)": "local"
        }
        
        # è·å–å½“å‰ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤å€¼
        current_provider = os.getenv("LLM_PROVIDER", "qwen")
        # åå‘æŸ¥æ‰¾å¯¹åº”çš„ index
        default_index = 0
        for idx, (name, key) in enumerate(model_options.items()):
            if key == current_provider:
                default_index = idx
                break
        
        selected_model_name = st.selectbox(
            "AIåç«¯",
            options=list(model_options.keys()),
            index=default_index,
            key="model_selector",
            help="é€‰æ‹©ç”¨äºè¯Šæ–­çš„åº•å±‚å¤§è¯­è¨€æ¨¡å‹",
            label_visibility="collapsed"
        )
        
        # æ›´æ–°ç¯å¢ƒå˜é‡
        selected_key = model_options[selected_model_name]
        os.environ["LLM_PROVIDER"] = selected_key

        # [step2-1] Ollama æ¨¡å‹é…ç½®
        if selected_key == "ollama":
            ollama_base = st.text_input(
                "Ollama åœ°å€",
                value=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                help="Ollama æœåŠ¡çš„ API åœ°å€"
            )
            os.environ["OLLAMA_BASE_URL"] = ollama_base
            
            ollama_model = st.text_input(
                "Ollama æ¨¡å‹åç§°",
                value=os.getenv("OLLAMA_MODEL", "FreedomIntelligence/HuatuGPT-7B"),
                placeholder="ä¾‹å¦‚: llama3, gemma:latest",
                help="è¯·è¾“å…¥å·²åœ¨ Ollama ä¸­ä¸‹è½½çš„æ¨¡å‹åç§°"
            )
            os.environ["OLLAMA_MODEL"] = ollama_model
            
            # æ˜¾ç¤ºçŠ¶æ€æ£€æŸ¥
            if st.button("æµ‹è¯• Ollama è¿æ¥", use_container_width=True):
                try:
                    import requests
                    # ä¸´æ—¶æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ä»¥é¿å… localhost è¿æ¥é—®é¢˜
                    proxies = {"http": None, "https": None}
                    resp = requests.get(ollama_base, timeout=2, proxies=proxies)
                    if resp.status_code == 200:
                        st.success("âœ… æœåŠ¡è¿æ¥æˆåŠŸ")
                        # æ£€æŸ¥æ¨¡å‹
                        try:
                            tags = requests.get(f"{ollama_base}/api/tags", timeout=2, proxies=proxies).json()
                            models = [m['name'] for m in tags.get('models', [])]
                            # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                            target = ollama_model.lower()
                            # å¤„ç† :latest åç¼€
                            if ":" not in target:
                                target += ":latest"
                            
                            found = False
                            for m in models:
                                m_lower = m.lower()
                                if target == m_lower:
                                    found = True
                                    break
                                # å°è¯•å¦‚æœä¸å¸¦ latest
                                if target.replace(":latest", "") == m_lower:
                                    found = True
                                    break
                                    
                            if found:
                                st.success(f"âœ… æ¨¡å‹ {ollama_model} å·²å°±ç»ª")
                            else:
                                st.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ {ollama_model}ï¼Œè¯·å…ˆæ‰§è¡Œ pull")
                                st.info(f"å¯ç”¨æ¨¡å‹: {', '.join(models)}")
                        except:
                            pass
                    else:
                        st.error(f"âŒ æœåŠ¡å¼‚å¸¸: {resp.status_code}")
                except Exception as e:
                    st.error(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama: {str(e)}")

        # [step2-2] HuggingFace æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®
        if selected_key == "local":
            local_path = st.text_input(
                "å¤§è¯­è¨€æ¨¡å‹è·¯å¾„ (LLM)",
                value=os.getenv("LOCAL_MODEL_PATH", ""),
                placeholder="ä¾‹å¦‚: models/qwen-7b-chat",
                help="è¯·è¾“å…¥æœ¬åœ° HuggingFace æ¨¡å‹ç›®å½•çš„ç»å¯¹è·¯å¾„"
            )
            if local_path:
                os.environ["LOCAL_MODEL_PATH"] = local_path
            else:
                st.warning("è¯·è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„")

            local_embedding = st.text_input(
                "Embedding æ¨¡å‹è·¯å¾„",
                value=os.getenv("LOCAL_EMBEDDING_MODEL", ""),
                placeholder="ä¾‹å¦‚: models/bge-small-zh",
                help="è¯·è¾“å…¥æœ¬åœ° Embedding æ¨¡å‹ç›®å½•çš„ç»å¯¹è·¯å¾„"
            )
            if local_embedding:
                os.environ["LOCAL_EMBEDDING_MODEL"] = local_embedding
        
        st.subheader("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        
        # [step3] çŸ¥è¯†åº“ç®¡ç†æŒ‰é’®
        # åˆ†ä¸¤ä¸ªæŒ‰é’®ï¼Œæ˜ç¡®åŠŸèƒ½åŒºåˆ†
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ”„ æ›´æ–°å‘é‡åº“", use_container_width=True, 
                        help="æ›´æ–° RAG æ£€ç´¢ç”¨çš„å‘é‡ç´¢å¼• (Pinecone/FAISS)"):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                    from src.scripts.ingest_knowledge import ingest_docs
                    status = ingest_docs()
                    if "æˆåŠŸ" in status:
                        st.toast(status, icon="âœ…")
                    else:
                        st.error(status)
        
        with col2:
            # Neo4j æŒ‰é’®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            from src.core.settings import get_settings
            settings = get_settings()
            
            if settings.enable_neo4j:
                if st.button("ğŸ•¸ï¸ æ›´æ–°å›¾è°±", use_container_width=True, 
                            help="æ›´æ–° Neo4j çŸ¥è¯†å›¾è°±ï¼ˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰"):
                    with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±..."):
                        try:
                            from src.scripts.build_kg import build_knowledge_graph
                            result = build_knowledge_graph()
                            if result and "æˆåŠŸ" in result:
                                st.toast(result, icon="âœ…")
                            else:
                                st.toast("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", icon="âœ…")
                        except Exception as e:
                            st.error(f"æ„å»ºå¤±è´¥: {str(e)}")
            else:
                st.button("ğŸ•¸ï¸ å›¾è°±æœªå¯ç”¨", use_container_width=True, disabled=True,
                         help="åœ¨é…ç½®ä¸­è®¾ç½® ENABLE_NEO4J=true ä»¥å¯ç”¨")
        
        # [step4] ç¼“å­˜æ¸…ç†
        if st.button("ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜", use_container_width=True,
                    help="æ¸…é™¤è¯Šæ–­ç»“æœç¼“å­˜ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´"):
            from src.services.cache import get_cache
            cache = get_cache()
            deleted_count = cache.clear_all()
            if deleted_count > 0:
                st.toast(f"å·²æ¸…é™¤ {deleted_count} æ¡ç¼“å­˜è®°å½•", icon="ğŸ—‘ï¸")
            else:
                st.toast("ç¼“å­˜å·²æ¸…ç©º", icon="âœ…")
        
