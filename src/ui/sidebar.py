import streamlit as st
import os

def render_sidebar():
    with st.sidebar:
        st.subheader("ğŸ¤– é€‰æ‹©å¤§æ¨¡å‹")
        
        # --- æ¨¡å‹åˆ‡æ¢åŠŸèƒ½ ---
        model_options = {
            "Qwen (é€šä¹‰åƒé—®)": "qwen",
            "OpenAI (GPT-3.5/4)": "openai",
            "Gemini (Google)": "gemini",
            "Local Model (æœ¬åœ°æ¨¡å‹)": "local"
        }
        
        # è·å–å½“å‰ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤å€¼
        current_provider = os.getenv("LLM_PROVIDER", "qwen")
        # åå‘æŸ¥æ‰¾å¯¹åº”çš„ index
        default_index = 0
        for idx, (name, key) in enumerate(model_options.items()):
            if key == current_provider:
                default_index = idx
                break
        
        selected_model_name = st.selectbox(
            "AIåç«¯",
            options=list(model_options.keys()),
            index=default_index,
            key="model_selector",
            help="é€‰æ‹©ç”¨äºè¯Šæ–­çš„åº•å±‚å¤§è¯­è¨€æ¨¡å‹",
            label_visibility="collapsed"
        )
        
        # æ›´æ–°ç¯å¢ƒå˜é‡
        selected_key = model_options[selected_model_name]
        os.environ["LLM_PROVIDER"] = selected_key

        # --- æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½® ---
        if selected_key == "local":
            local_path = st.text_input(
                "æœ¬åœ°æ¨¡å‹è·¯å¾„",
                value=os.getenv("LOCAL_MODEL_PATH", ""),
                placeholder="ä¾‹å¦‚: models/qwen-7b-chat",
                help="è¯·è¾“å…¥æœ¬åœ° HuggingFace æ¨¡å‹ç›®å½•çš„ç»å¯¹è·¯å¾„"
            )
            if local_path:
                os.environ["LOCAL_MODEL_PATH"] = local_path
            else:
                st.warning("è¯·è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„")
        
        st.subheader("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        
        # åˆ†ä¸¤ä¸ªæŒ‰é’®ï¼Œæ˜ç¡®åŠŸèƒ½åŒºåˆ†
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ”„ æ›´æ–°å‘é‡åº“", use_container_width=True, 
                        help="æ›´æ–° RAG æ£€ç´¢ç”¨çš„å‘é‡ç´¢å¼• (Pinecone/FAISS)"):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                    from src.scripts.ingest_knowledge import ingest_docs
                    status = ingest_docs()
                    if "æˆåŠŸ" in status:
                        st.toast(status, icon="âœ…")
                    else:
                        st.error(status)
        
        with col2:
            # Neo4j æŒ‰é’®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            from src.core.settings import get_settings
            settings = get_settings()
            
            if settings.enable_neo4j:
                if st.button("ğŸ•¸ï¸ æ›´æ–°å›¾è°±", use_container_width=True, 
                            help="æ›´æ–° Neo4j çŸ¥è¯†å›¾è°±ï¼ˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰"):
                    with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±..."):
                        try:
                            from src.scripts.build_kg import build_knowledge_graph
                            result = build_knowledge_graph()
                            if result and "æˆåŠŸ" in result:
                                st.toast(result, icon="âœ…")
                            else:
                                st.toast("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", icon="âœ…")
                        except Exception as e:
                            st.error(f"æ„å»ºå¤±è´¥: {str(e)}")
            else:
                st.button("ğŸ•¸ï¸ å›¾è°±æœªå¯ç”¨", use_container_width=True, disabled=True,
                         help="åœ¨é…ç½®ä¸­è®¾ç½® ENABLE_NEO4J=true ä»¥å¯ç”¨")
        
        # æ¸…é™¤ç¼“å­˜æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜", use_container_width=True,
                    help="æ¸…é™¤è¯Šæ–­ç»“æœç¼“å­˜ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´"):
            from src.services.cache import get_cache
            cache = get_cache()
            deleted_count = cache.clear_all()
            if deleted_count > 0:
                st.toast(f"å·²æ¸…é™¤ {deleted_count} æ¡ç¼“å­˜è®°å½•", icon="ğŸ—‘ï¸")
            else:
                st.toast("ç¼“å­˜å·²æ¸…ç©º", icon="âœ…")
        
