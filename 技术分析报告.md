# 医疗诊断 AI 智能体项目 - 技术分析报告

## 问题一：为什么用这些技术？

### 1. **LangChain**
**原因：**
- **统一 LLM 接口**：项目需要支持 Qwen、OpenAI、Gemini 多个模型，LangChain 提供统一的 `ChatModel` 接口，避免为每个模型写不同调用代码
- **链式编排**：多智能体协作需要将 Prompt → LLM → 工具调用串联，LangChain 的 `Chain` 和 `LCEL` 语法简化了流程编排
- **生态集成**：LangChain 与 Pinecone、FAISS 等向量库无缝集成，RAG 实现更便捷
- **自动重试与容错**：LangChain 内置 `with_fallbacks()` 机制，实现多模型自动切换

### 2. **asyncio（异步编程）**
**原因：**
- **并发执行多专科诊断**：13 个专科智能体需要同时分析病例，串行执行会耗时过长（13 × 5秒 = 65秒），异步并发可将总时间缩短至 5-10 秒
- **流式返回结果**：使用 `asyncio.as_completed()` 实现"谁先完成谁先返回"，提升用户体验（无需等待所有专科完成）
- **非阻塞 I/O**：LLM API 调用是网络 I/O 操作，异步避免阻塞主线程，提高系统吞吐量

### 3. **Qwen（通义千问）**
**原因：**
- **中文医疗场景优化**：Qwen 对中文医学术语理解更准确，适合处理中文医疗报告
- **成本效益**：相比 GPT-4，Qwen 价格更低，适合大规模部署
- **API 稳定性**：DashScope API 在国内访问速度快，延迟低
- **多模型支持**：项目同时支持 Qwen/OpenAI/Gemini，Qwen 作为默认主模型，其他作为备用

### 4. **Prompt Engineering（提示工程）**
**原因：**
- **角色专业化**：每个专科医生需要不同的 Prompt 模板（心脏科关注心电图，消化科关注胃肠镜），通过 YAML 配置文件管理 13+ 个专业 Prompt
- **输出格式控制**：通过精心设计的 Prompt 确保 LLM 输出结构化诊断报告（问题+理由+建议），而非自由文本
- **上下文注入**：Prompt 中动态注入 RAG 检索的医学知识片段，提升诊断准确性

### 5. **ReAct（推理-行动范式）**
**原因：**
- **结构化推理**：多学科团队需要综合多个专科意见，ReAct 的 "Thought → Action → Observation" 循环帮助模型逐步推理
- **工具调用能力**：ReAct 允许 LLM 调用 `generate_structured_diagnosis` 工具，将自由文本转换为结构化 JSON（issues 数组）
- **可解释性**：ReAct 的思考过程（thought）可记录，便于医生审查诊断逻辑

### 6. **RAG（检索增强生成）**
**原因：**
- **知识库增强**：LLM 的医学知识可能过时或不完整，RAG 从本地 100+ 疾病知识库检索最新指南，注入到 Prompt 中
- **减少幻觉**：通过检索真实医学文献，降低 LLM 编造错误诊断的风险
- **可扩展性**：新增疾病只需更新知识库文件，无需重新训练模型

### 7. **Pinecone（向量数据库）**
**原因：**
- **语义检索**：传统关键词搜索无法匹配"胸痛"和"心绞痛"的语义关联，向量检索通过 Embedding 实现语义相似度匹配
- **大规模知识库**：100+ 疾病文档，每个文档切分为多个 chunk，向量库支持百万级向量快速检索（毫秒级）
- **云端托管**：Pinecone 是托管服务，无需自建向量数据库基础设施，降低运维成本

### 8. **Docker Compose（容器化部署）**
**原因：**
- **环境一致性**：开发、测试、生产环境统一，避免"在我机器上能跑"问题
- **依赖隔离**：Python 依赖、环境变量（API Key）封装在容器内，不污染宿主机
- **一键部署**：`docker-compose up` 即可启动整个系统，简化部署流程
- **可扩展性**：未来可轻松添加 Redis、PostgreSQL 等服务

---

## 问题二：在项目中什么地方用到了这些技术？

### 1. **LangChain**

#### 使用位置：
- **`src/services/llm.py`** (第 4-6 行)：
  ```python
  from langchain_community.chat_models import ChatTongyi
  from langchain_openai import ChatOpenAI
  from langchain_google_genai import ChatGoogleGenerativeAI
  ```
  使用 LangChain 的模型封装类初始化 Qwen、OpenAI、Gemini

- **`src/services/llm.py`** (第 165 行)：
  ```python
  return primary_model.with_fallbacks(fallback_models)
  ```
  使用 LangChain 的 `with_fallbacks()` 实现多模型自动切换

- **`src/agents/base.py`** (第 10 行)：
  ```python
  from langchain_core.prompts import PromptTemplate
  ```
  使用 LangChain 的 `PromptTemplate` 构建提示词模板

- **`src/core/triage.py`** (第 31 行)：
  ```python
  chain = prompt | llm
  response = await chain.ainvoke({...})
  ```
  使用 LangChain 的 LCEL（LangChain Expression Language）链式调用

- **`src/services/rag.py`** (第 17 行)：
  ```python
  from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings
  ```
  使用 LangChain 的 Pinecone 集成进行向量检索

- **`src/scripts/ingest_knowledge.py`** (第 27-29 行)：
  ```python
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  from langchain_core.documents import Document
  from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings
  ```
  使用 LangChain 的文本分割器和文档类进行知识入库

### 2. **asyncio（异步编程）**

#### 使用位置：
- **`src/core/orchestrator.py`** (第 15 行)：
  ```python
  import asyncio
  ```

- **`src/core/orchestrator.py`** (第 25 行)：
  ```python
  async def generate_diagnosis(medical_report: str):
  ```
  主诊断流程使用异步函数

- **`src/core/orchestrator.py`** (第 93-96 行)：
  ```python
  async def wrapped_run(name, agent):
      res = await agent.run_async()
      return name, res
  ```
  包装异步任务

- **`src/core/orchestrator.py`** (第 99-107 行)：
  ```python
  wrapped_tasks = [wrapped_run(name, agent) for name, agent in agents.items()]
  for coro in asyncio.as_completed(wrapped_tasks):
      agent_name, response = await coro
      yield agent_name, response
  ```
  使用 `asyncio.as_completed()` 实现并发执行和流式返回

- **`src/agents/base.py`** (第 70 行)：
  ```python
  async def run_async(self):
      response = await self.model.ainvoke(prompt)
  ```
  Agent 的异步执行方法

- **`app.py`** (第 14 行)：
  ```python
  import asyncio
  ```

- **`app.py`** (第 148 行)：
  ```python
  async def run_async_diagnosis():
  ```

- **`app.py`** (第 205 行)：
  ```python
  asyncio.run(run_async_diagnosis())
  ```
  在 Streamlit 中运行异步任务

### 3. **Qwen（通义千问）**

#### 使用位置：
- **`src/services/llm.py`** (第 4 行)：
  ```python
  from langchain_community.chat_models import ChatTongyi
  ```

- **`src/services/llm.py`** (第 78-85 行)：
  ```python
  if os.getenv("DASHSCOPE_API_KEY"):
      try:
          model_name = os.getenv("QWEN_MODEL", "qwen-max")
          available_models["qwen"] = ChatTongyi(model=model_name, temperature=temperature)
  ```
  初始化 Qwen 模型

- **`src/services/llm.py`** (第 117 行)：
  ```python
  provider = os.getenv("LLM_PROVIDER", "qwen").lower()
  ```
  Qwen 作为默认主模型

- **`app.py`** (第 250-252 行)：
  ```python
  api_key = os.getenv("DASHSCOPE_API_KEY")
  base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  model = os.getenv("QWEN_MODEL", "qwen-max")
  ```
  聊天组件默认使用 Qwen

### 4. **Prompt Engineering（提示工程）**

#### 使用位置：
- **`config/prompts.yaml`** (整个文件)：
  包含 13 个专科医生的 Prompt 模板和多学科团队的 Prompt

- **`src/agents/base.py`** (第 19-27 行)：
  ```python
  def load_prompts():
      with open("config/prompts.yaml", "r", encoding="utf-8") as f:
          return yaml.safe_load(f)
  ```
  加载 YAML 配置文件

- **`src/agents/base.py`** (第 37-47 行)：
  ```python
  def create_prompt_template(self):
      specialist_prompts = PROMPTS_CONFIG.get("specialists", {})
      template = specialist_prompts.get(self.role, "")
      return PromptTemplate.from_template(template)
  ```
  根据角色动态加载对应的 Prompt 模板

- **`src/agents/base.py`** (第 52 行)：
  ```python
  prompt = self.prompt_template.format(medical_report=self.medical_report)
  ```
  填充 Prompt 模板变量

- **`src/agents/base.py`** (第 95-123 行)：
  ```python
  def create_prompt_template(self):
      # 动态构建多学科团队的 Prompt
      reports_text = "\n".join(active_reports)
      specialists_text = "、".join(active_specialists)
      return PromptTemplate.from_template(template_str).partial(...)
  ```
  多学科团队 Prompt 的动态构建

- **`src/core/triage.py`** (第 12-28 行)：
  ```python
  prompt_template = """
  你是一位经验丰富的全科分诊医生。
  请阅读以下患者的医疗报告，并从给定的专科医生列表中，挑选出最需要参与会诊的科室。
  ...
  """
  ```
  分诊 Prompt 模板

### 5. **ReAct（推理-行动范式）**

#### 使用位置：
- **`src/core/orchestrator.py`** (第 117 行)：
  ```python
  final_diagnosis = await team_agent.run_react_async()
  ```
  调用 ReAct 模式生成最终诊断

- **`src/agents/base.py`** (第 125-268 行)：
  ```python
  async def run_react_async(self, max_steps: int = 2):
      history = []
      observation = None
      
      for _ in range(max_steps):
          prompt = (
              "你是一支多学科医疗团队，正在使用 ReAct 策略进行推理。"
              "请只输出一个 JSON，对象格式如下："
              '{"thought": "...", "tool": "...", "args": {...}, "final_answer": "..."}'
          )
          # ... 解析 JSON，执行工具调用，更新 observation
  ```
  完整的 ReAct 实现：Thought → Tool → Observation 循环

- **`src/agents/base.py`** (第 151-168 行)：
  ```python
  prompt = (
      "你是一支多学科医疗团队，正在使用 ReAct 策略进行推理。"
      "请只输出一个 JSON，对象格式如下："
      '{"thought": "当前一步的思考", "tool": "generate_structured_diagnosis" 或 null, ...}'
  )
  ```
  ReAct Prompt 设计

- **`src/agents/base.py`** (第 198-216 行)：
  ```python
  thought = data.get("thought")
  tool_name = data.get("tool")
  history.append({"thought": thought, "tool": tool_name})
  
  if tool_name == "generate_structured_diagnosis":
      tool_call = json.dumps({"tool": tool_name, "args": tool_args}, ensure_ascii=False)
      observation = execute_tool_call(tool_call)
  ```
  ReAct 的执行逻辑：解析思考、调用工具、获取观察结果

### 6. **RAG（检索增强生成）**

#### 使用位置：
- **`src/services/rag.py`** (整个文件)：
  RAG 模块的完整实现

- **`src/services/rag.py`** (第 94-121 行)：
  ```python
  def retrieve_knowledge_snippets(query: str, k: int = 3) -> str:
      vectorstore = _get_vectorstore()
      docs = vectorstore.similarity_search(query, k=k)
      # 拼接检索到的知识片段
      return "\n".join(snippets)
  ```
  核心检索函数

- **`src/agents/base.py`** (第 13 行)：
  ```python
  from src.services.rag import retrieve_knowledge_snippets
  ```

- **`src/agents/base.py`** (第 54-61 行)：
  ```python
  if self.medical_report:
      rag_context = retrieve_knowledge_snippets(self.medical_report)
      if rag_context:
          prompt = (
              "以下是与患者情况相关的医学知识片段（供你参考，不必逐条复述）：\n"
              f"{rag_context}\n\n"
              "在参考以上知识的基础上，回答下面的任务：\n"
              f"{prompt}"
          )
  ```
  在 Agent 执行前检索知识并注入到 Prompt 中

- **`src/scripts/ingest_knowledge.py`** (整个文件)：
  知识入库脚本，将医学知识文档向量化并存入 Pinecone

### 7. **Pinecone（向量数据库）**

#### 使用位置：
- **`src/services/rag.py`** (第 17 行)：
  ```python
  from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings
  ```

- **`src/services/rag.py`** (第 26-34 行)：
  ```python
  def _get_embedding_model() -> PineconeEmbeddings:
      model_name = os.getenv("PINECONE_EMBEDDING_MODEL", "llama-text-embed-v2")
      return PineconeEmbeddings(model=model_name)
  ```
  初始化 Pinecone Embedding 模型

- **`src/services/rag.py`** (第 37-91 行)：
  ```python
  def _get_vectorstore() -> PineconeVectorStore | None:
      api_key = os.getenv("PINECONE_API_KEY")
      index_name = os.getenv("PINECONE_INDEX_NAME", "medical-knowledge")
      embedding = _get_embedding_model()
      vectorstore = PineconeVectorStore.from_existing_index(
          index_name=index_name,
          embedding=embedding,
      )
      return vectorstore
  ```
  连接 Pinecone 向量库

- **`src/services/rag.py`** (第 109 行)：
  ```python
  docs = vectorstore.similarity_search(query, k=k)
  ```
  执行向量相似度搜索

- **`src/scripts/ingest_knowledge.py`** (第 29 行)：
  ```python
  from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings
  ```

- **`src/scripts/ingest_knowledge.py`** (第 95-112 行)：
  ```python
  embedding_model = PineconeEmbeddings(
      model=os.getenv("PINECONE_EMBEDDING_MODEL", "llama-text-embed-v2"),
  )
  PineconeVectorStore.from_documents(
      documents=batch,
      embedding=embedding_model,
      index_name=index_name,
  )
  ```
  将知识文档向量化并写入 Pinecone

### 8. **Docker Compose（容器化部署）**

#### 使用位置：
- **`Dockerfile`** (整个文件)：
  ```dockerfile
  FROM python:3.11-slim
  WORKDIR /app
  COPY requirements.txt ./
  RUN pip install --no-cache-dir -r requirements.txt
  COPY . .
  CMD ["python", "Main.py"]
  ```
  定义容器镜像构建规则

- **`docker-compose.yml`** (整个文件)：
  ```yaml
  version: "3.9"
  services:
    app:
      build: .
      container_name: medical-diagnostics-agent
      volumes:
        - "./Results:/app/results"
        - "./Medical Reports:/app/Medical Reports"
        - "./KnowledgeBase:/app/KnowledgeBase"
      env_file:
        - apikey.env
      working_dir: /app
      command: ["python", "Main.py"]
  ```
  定义服务编排、卷挂载、环境变量加载

---

## 问题三：这些技术的原理是什么？

### 1. **LangChain 原理**

**核心概念：**
- **Chain（链）**：将多个组件（Prompt、LLM、工具）串联成执行流程
- **LCEL（LangChain Expression Language）**：使用 `|` 操作符连接组件，如 `prompt | llm | parser`
- **统一接口**：所有 LLM 都实现 `BaseChatModel` 接口，提供 `invoke()` 和 `ainvoke()` 方法

**工作流程：**
```
用户输入 → PromptTemplate（填充变量）→ ChatModel（调用 LLM API）→ 输出解析器 → 最终结果
```

**在本项目中的应用：**
- `src/core/triage.py` 中的 `chain = prompt | llm` 就是 LCEL 语法
- `src/services/llm.py` 中的 `with_fallbacks()` 实现了链式容错：主模型失败 → 自动切换到备用模型

### 2. **asyncio 原理**

**核心概念：**
- **协程（Coroutine）**：使用 `async def` 定义的函数，返回协程对象
- **事件循环（Event Loop）**：管理所有异步任务的调度和执行
- **并发 vs 并行**：asyncio 是单线程并发（I/O 等待时切换任务），不是多线程并行

**工作流程：**
```
主线程启动事件循环
  ↓
创建多个协程任务（13 个专科 Agent）
  ↓
事件循环调度：当某个 Agent 等待 API 响应时，切换到其他 Agent
  ↓
使用 asyncio.as_completed() 按完成顺序返回结果（流式输出）
```

**在本项目中的应用：**
- `src/core/orchestrator.py` 中，13 个专科 Agent 并发执行，总耗时 ≈ 最慢的那个 Agent（而非 13 倍）
- `asyncio.as_completed()` 实现"谁先完成谁先返回"，提升用户体验

### 3. **Qwen 原理**

**核心概念：**
- **Transformer 架构**：基于注意力机制的大语言模型
- **API 调用**：通过 HTTP 请求调用 DashScope API，发送 Prompt，接收生成的文本
- **流式输出**：支持 Server-Sent Events (SSE) 流式返回，实现打字机效果

**工作流程：**
```
用户 Prompt → HTTP POST 请求 → DashScope API 服务器
  ↓
Qwen 模型推理（Transformer 前向传播）
  ↓
生成 Token 序列 → 返回 JSON 响应
  ↓
解析 content 字段 → 返回给应用
```

**在本项目中的应用：**
- `ChatTongyi` 封装了 HTTP 请求逻辑，自动处理认证、重试、错误处理
- 支持 `temperature`、`max_tokens` 等参数控制生成质量

### 4. **Prompt Engineering 原理**

**核心概念：**
- **Few-Shot Learning**：在 Prompt 中提供示例，引导 LLM 学习输出格式
- **角色扮演**：通过 "你是一位心脏科医生" 等指令，让 LLM 模拟专业角色
- **结构化输出**：通过明确的格式要求（JSON、列表），控制 LLM 输出结构

**Prompt 设计模式：**
```
角色定义 + 任务描述 + 输入数据 + 输出格式要求 + 示例
```

**在本项目中的应用：**
- `config/prompts.yaml` 中每个专科 Prompt 都包含：角色、任务、重点、建议、输出格式
- `src/core/triage.py` 中的分诊 Prompt 要求返回 JSON 数组，通过示例引导格式

### 5. **ReAct 原理**

**核心概念：**
- **ReAct = Reasoning + Acting**：将推理和行动分离
- **循环结构**：`Thought（思考）→ Action（行动）→ Observation（观察）→ Thought → ...`
- **工具调用**：LLM 可以调用外部工具（如 `generate_structured_diagnosis`）获取结构化数据

**工作流程：**
```
Step 1: LLM 思考当前状态，决定调用哪个工具
  ↓
Step 2: 执行工具调用，获取观察结果（Observation）
  ↓
Step 3: 基于观察结果，继续思考或输出最终答案
  ↓
重复直到满足终止条件（max_steps 或 final_answer 非空）
```

**在本项目中的应用：**
- `src/agents/base.py` 的 `run_react_async()` 实现了完整的 ReAct 循环
- LLM 输出 JSON：`{"thought": "...", "tool": "generate_structured_diagnosis", "args": {...}}`
- 工具返回结构化诊断：`{"result": {"issues": [...]}}`
- LLM 基于工具结果生成最终诊断文本

### 6. **RAG 原理**

**核心概念：**
- **Embedding（向量化）**：将文本转换为高维向量（如 768 维），语义相似的文本向量距离更近
- **向量检索**：使用余弦相似度或点积计算查询向量与知识库向量的相似度，返回 Top-K 最相似的文档
- **上下文注入**：将检索到的知识片段拼接到 Prompt 中，作为 LLM 的额外上下文

**工作流程：**
```
知识库文档 → 文本分割（chunk）→ Embedding 模型向量化 → 存入向量数据库
  ↓
用户查询 → Embedding 模型向量化 → 向量相似度搜索 → 检索 Top-K 相关文档
  ↓
检索结果拼接 → 注入到 Prompt → LLM 基于增强上下文生成回答
```

**在本项目中的应用：**
- `src/scripts/ingest_knowledge.py` 将 100+ 疾病文档切分、向量化、存入 Pinecone
- `src/services/rag.py` 的 `retrieve_knowledge_snippets()` 执行检索
- `src/agents/base.py` 在 Agent 执行前调用 RAG，将知识注入 Prompt

### 7. **Pinecone 原理**

**核心概念：**
- **向量数据库**：专门存储和检索高维向量的数据库（不同于传统关系型数据库）
- **近似最近邻搜索（ANN）**：使用 HNSW（分层导航小世界）等算法，在百万级向量中快速找到最相似的 Top-K
- **托管服务**：Pinecone 是云端 SaaS，无需自建基础设施

**工作流程：**
```
文档向量 → HTTP API 上传 → Pinecone 云端索引（HNSW 图结构）
  ↓
查询向量 → HTTP API 搜索 → Pinecone 计算相似度 → 返回 Top-K 向量 ID 和元数据
```

**在本项目中的应用：**
- `PineconeVectorStore.from_existing_index()` 连接到已有索引
- `vectorstore.similarity_search(query, k=3)` 执行语义搜索，返回 3 条最相关的医学知识

### 8. **Docker Compose 原理**

**核心概念：**
- **容器化**：将应用及其依赖打包成镜像，在隔离环境中运行
- **服务编排**：通过 YAML 文件定义多个服务（如 app、redis、db）的配置和依赖关系
- **卷挂载**：将宿主机目录挂载到容器内，实现数据持久化

**工作流程：**
```
docker-compose.yml 定义服务配置
  ↓
docker-compose up 解析 YAML → 构建镜像（如需要）→ 创建网络 → 启动容器
  ↓
容器内运行应用，通过卷挂载访问宿主机文件，通过环境变量注入配置
```

**在本项目中的应用：**
- `Dockerfile` 定义 Python 环境、依赖安装、代码复制
- `docker-compose.yml` 定义服务、卷挂载（知识库、报告目录）、环境变量文件（API Key）
- 运行 `docker-compose up` 一键启动整个系统

---

## 总结

本项目是一个典型的 **Multi-Agent + RAG + ReAct** 架构的医疗 AI 系统：

1. **LangChain** 提供统一的 LLM 接口和链式编排能力
2. **asyncio** 实现多智能体并发执行，提升响应速度
3. **Qwen** 作为主模型，提供中文医疗场景的优质推理能力
4. **Prompt Engineering** 通过精心设计的模板，引导 LLM 输出专业诊断
5. **ReAct** 实现结构化推理，将自由文本转换为结构化诊断报告
6. **RAG** 从医学知识库检索相关指南，增强 LLM 的准确性
7. **Pinecone** 提供高效的向量检索能力，支持大规模知识库
8. **Docker Compose** 实现一键部署，保证环境一致性

这些技术相互配合，共同构建了一个高效、准确、可扩展的医疗诊断 AI 系统。

